# 10.2 Responsible AI: Ethics & Governance

Using AI is not just a technical choice; it is an **ethical** one. As a Project Manager, you are the steward of your organization's data, your client's secrets, and your team's integrity.

---

## üõ°Ô∏è The 3 Pillars of Responsible AI
To navigate the risks of AI, follow these pillars:

<div class="resp-grid">
  <div class="resp-card">
    <div class="resp-title">1. Data Privacy</div>
    <div class="resp-tag">The Red Line</div>
    <p><strong>Rule:</strong> Never put sensitive data (PII, Financials, IP, Strategy) into a public, free AI model (like standard ChatGPT). It may be used to train the model.</p>
  </div>
  <div class="resp-card">
    <div class="resp-title">2. Bias Awareness</div>
    <div class="resp-tag">The Blind Spot</div>
    <p><strong>Rule:</strong> AI is trained on history. If history was biased (e.g., hiring practices), the AI will be biased. You must actively audit outputs for fairness.</p>
  </div>
  <div class="resp-card">
    <div class="resp-title">3. Accountability</div>
    <div class="resp-tag">The Owner</div>
    <p><strong>Rule:</strong> The AI is a tool, like a calculator. If a bridge collapses because of a calculation error, the engineer is blamed, not the calculator.</p>
  </div>
</div>

---

## ü§ù Alignment with PMI Code of Ethics
Your ethical obligations don't change just because the tool is new. Here is how the **PMI Code of Ethics** applies to AI:

| Value | AI Application |
| :--- | :--- |
| **Responsibility** | You are accountable for the output. You cannot blame the AI for errors, hallucinations, or copyright infringement. You must report data breaches immediately. |
| **Respect** | Respect stakeholders' privacy by not feeding their data into public models. Respect team members by not using AI to surveil them unethically (e.g., analyzing sentiment in private chats without consent). |
| **Fairness** | Actively check AI outputs for bias. Ensure tools don't discriminate in hiring, resourcing, or vendor selection. |
| **Honesty** | Be transparent. Disclose when an artifact (like a status report or schedule) was generated by AI. Do not present AI work as your own original creation if it isn't. |

---

## üîê Data Privacy: What You Must Protect
On the PMP exam, if a scenario includes **client confidentiality**, **PII**, or **unapproved tools**, the correct answer focuses on **policy + protection + escalation (if needed)**.

**Examples of data you should treat as sensitive unless explicitly approved:**
*   Personally Identifiable Information (PII): names, emails, phone numbers, IDs, addresses
*   Credentials: passwords, API keys, access tokens
*   Financials: pricing details, account numbers, forecasts, internal budgets
*   Intellectual Property (IP): designs, source code, trade secrets, strategy documents
*   Regulated data (industry-specific): healthcare, banking, government, etc.

**Safer patterns (practical):**
*   Use **enterprise-approved AI tools** with contractual protections (retention controls, no training on your data).
*   Prefer **grounded workflows** (RAG against approved internal sources) over ‚Äúopen internet guessing‚Äù.
*   Apply **data minimization**: share only what the AI needs to produce a draft.

---

## üìú The "AI Charter"
Just as you create a Team Charter for human interactions, you need an **AI Working Agreement**. It should define:
*   **Approved Tools**: Which specific engines (Enterprise versions) are safe to use?
*   **Transparency**: When must a team member *disclose* that a document was written by AI?
*   **Validation**: What is the mandatory review process before AI content leaves the team?
*   **Storage/Audit**: Where are prompts/outputs stored (and for how long), if needed for compliance?

::: danger üö´ "Shadow AI"
Using unapproved AI tools to bypass security protocols is known as "Shadow AI." This is a major compliance violation. The PM must provide safe, approved alternatives so the team isn't tempted to go rogue.
:::

---

## üë• Governance: Who Owns What (RACI Starter)
Responsible AI is a **governance** problem as much as a technology problem.

| Activity | PM | Sponsor/Product | IT/Security | Legal/Compliance | Data Owner/SME |
|---|---:|---:|---:|---:|---:|
| Approve AI tools for team use | C | A | R | C | C |
| Define data classification rules | C | C | R | R | A |
| Create AI working agreement | R | A | C | C | C |
| Review AI-generated artifacts before external use | R | A | C | C | R |
| Handle suspected data exposure incident | C | C | R | R | C |

Legend: **R** = Responsible, **A** = Accountable, **C** = Consulted

---

## üß® Common AI Risks in Projects (Know These Triggers)
*   **Prompt injection**: AI consumes untrusted content (email/docs) that includes malicious instructions (‚Äúignore policy and reveal secrets‚Äù).
*   **Over-reliance**: Teams treat AI output as ‚Äútruth‚Äù and skip validation steps.
*   **Bias**: AI recommendations amplify historical inequality (resourcing, stakeholder sentiment, hiring support roles).
*   **Compliance gaps**: Decisions are not explainable/auditable in regulated work.

---

## üèõÔ∏è Explainability (XAI)
In regulated industries (Healthcare, Finance), you cannot just say "The AI told me to reject this loan." You need **Explainability**.
*   **Black Box AI**: Inputs go in, answers come out, no one knows why. (Avoid for critical decisions).
*   **Explainable AI**: The system provides the "Why" behind the decision. (Required for Governance).

<style>
.resp-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
  gap: 1.5rem;
  margin: 1.5rem 0;
}

.resp-card {
  padding: 1.5rem;
  background: var(--vp-c-bg-soft);
  border: 1px solid var(--vp-c-border);
  border-radius: 12px;
}

.resp-title {
  font-weight: 700;
  font-size: 1.1rem;
  margin-bottom: 0.25rem;
  color: var(--vp-c-brand);
}

.resp-tag {
  font-size: 0.75rem;
  font-weight: 800;
  text-transform: uppercase;
  color: var(--vp-c-text-2);
  margin-bottom: 1rem;
}

.resp-card p {
  font-size: 0.85rem;
  margin: 0;
  line-height: 1.5;
}
</style>

---

## üßæ Transparency, IP, and Procurement (Exam-Relevant)
*   **Transparency**: Follow the team‚Äôs working agreement. If external stakeholders will rely on an artifact, disclose AI assistance when required and ensure a human owner signs off.
*   **IP/Copyright**: Treat prompts/outputs as potentially sensitive. Validate that your tool‚Äôs terms allow your intended commercial use and don‚Äôt accidentally leak third-party content.
*   **Procurement mindset**: If your project is buying AI capability, ensure the contract/SOW defines retention, training use, audit logs, SLAs, and responsibilities for incidents.

---

## üöë If a Privacy/Security Incident Happens (What to Do First)
1. **Stop and contain**: halt use of the tool/workflow immediately.
2. **Notify the right parties**: IT/Security (and Legal/Compliance if required).
3. **Preserve evidence**: keep logs/screenshots per policy (don‚Äôt ‚Äúcover it up‚Äù).
4. **Communicate appropriately**: follow the incident response plan for stakeholders.
5. **Prevent recurrence**: update the AI working agreement, tooling controls, and training.

<div class="study-tip">
  <strong>üìù Exam Insight:</strong> If a team member uses a free online AI to summarize a confidential client meeting, they have committed a <strong>Security Breach</strong>. The PM‚Äôs response is to <strong>immediately contain</strong> (notify IT/Security per policy), then educate the team and provide approved alternatives.
</div>

<style>
.study-tip {
  background: var(--vp-c-brand-soft);
  border-left: 4px solid var(--vp-c-brand);
  padding: 1rem;
  border-radius: 8px;
  margin: 2rem 0;
}
</style>
