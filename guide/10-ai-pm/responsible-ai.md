<script setup>
import ConceptGrid from '../../.vitepress/theme/components/ConceptGrid.vue'
import ConceptCard from '../../.vitepress/theme/components/ConceptCard.vue'
</script>

# 10.2 Responsible AI: Ethics & Governance

**ECO Task**: Plan and manage project compliance

Using AI is not just a technical choice; it is an **ethical** one. As a Project Manager, you are the steward of your organization's data, your client's secrets, and your team's integrity.

---

## The 3 Pillars of Responsible AI
To navigate the risks of AI, follow these pillars:

<ConceptGrid>
  <ConceptCard title="1. Data Privacy" tag="The Red Line">
    <strong>Rule:</strong> Never put sensitive data (PII, Financials, IP, Strategy) into a public, free AI model (like standard ChatGPT). It may be used to train the model.
  </ConceptCard>
  <ConceptCard title="2. Bias Awareness" tag="The Blind Spot">
    <strong>Rule:</strong> AI is trained on history. If history was biased (e.g., hiring practices), the AI will be biased. You must actively audit outputs for fairness.
  </ConceptCard>
  <ConceptCard title="3. Accountability" tag="The Owner">
    <strong>Rule:</strong> The AI is a tool, like a calculator. If a bridge collapses because of a calculation error, the engineer is blamed, not the calculator.
  </ConceptCard>
</ConceptGrid>

---

## Alignment with PMI Code of Ethics
Your ethical obligations don't change just because the tool is new. Here is how the **PMI Code of Ethics** applies to AI:

| Value | AI Application |
| :--- | :--- |
| **Responsibility** | You are accountable for the output. You cannot blame the AI for errors, hallucinations, or copyright infringement. You must report data breaches immediately. |
| **Respect** | Respect stakeholders' privacy by not feeding their data into public models. Respect team members by not using AI to surveil them unethically (e.g., analyzing sentiment in private chats without consent). |
| **Fairness** | Actively check AI outputs for bias. Ensure tools don't discriminate in hiring, resourcing, or vendor selection. |
| **Honesty** | Be transparent. Disclose when an artifact (like a status report or schedule) was generated by AI. Do not present AI work as your own original creation if it isn't. |

---

## Data Privacy: What You Must Protect
On the PMP exam, if a scenario includes **client confidentiality**, **PII**, or **unapproved tools**, the correct answer focuses on **policy + protection + escalation (if needed)**.

<strong>Examples of data you should treat as sensitive unless explicitly approved:</strong>
*   Personally Identifiable Information (PII): names, emails, phone numbers, IDs, addresses
*   Credentials: passwords, API keys, access tokens
*   Financials: pricing details, account numbers, forecasts, internal budgets
*   Intellectual Property (IP): designs, source code, trade secrets, strategy documents
*   Regulated data (industry-specific): healthcare, banking, government, etc.

**Safer patterns (practical):**
*   Use **enterprise-approved AI tools** with contractual protections (retention controls, no training on your data).
*   Prefer **grounded workflows** (RAG against approved internal sources) over “open internet guessing”.
*   Apply **data minimization**: share only what the AI needs to produce a draft.

---

## The "AI Charter"
Just as you create a Team Charter for human interactions, you need an **AI Working Agreement**. It should define:
*   **Approved Tools**: Which specific engines (Enterprise versions) are safe to use?
*   **Transparency**: When must a team member *disclose* that a document was written by AI?
*   **Validation**: What is the mandatory review process before AI content leaves the team?
*   **Storage/Audit**: Where are prompts/outputs stored (and for how long), if needed for compliance?

::: danger "Shadow AI"
Using unapproved AI tools to bypass security protocols is known as "Shadow AI." This is a major compliance violation. The PM must provide safe, approved alternatives so the team isn't tempted to go rogue.
:::

---

## Governance: Who Owns What (RACI Starter)
Responsible AI is a **governance** problem as much as a technology problem.

| Activity | PM | Sponsor/Product | IT/Security | Legal/Compliance | Data Owner/SME |
|---|---:|---:|---:|---:|---:|
| Approve AI tools for team use | C | A | R | C | C |
| Define data classification rules | C | C | R | R | A |
| Create AI working agreement | R | A | C | C | C |
| Review AI-generated artifacts before external use | R | A | C | C | R |
| Handle suspected data exposure incident | C | C | R | R | C |

Legend: **R** = Responsible, **A** = Accountable, **C** = Consulted

---

## Common AI Risks in Projects (Know These Triggers)
*   **Prompt injection**: AI consumes untrusted content (email/docs) that includes malicious instructions (“ignore policy and reveal secrets”).
*   **Over-reliance**: Teams treat AI output as “truth” and skip validation steps.
*   **Bias**: AI recommendations amplify historical inequality (resourcing, stakeholder sentiment, hiring support roles).
*   **Compliance gaps**: Decisions are not explainable/auditable in regulated work.

---

## Explainability (XAI)
In regulated industries (Healthcare, Finance), you cannot just say "The AI told me to reject this loan." You need **Explainability**.
*   **Black Box AI**: Inputs go in, answers come out, no one knows why. (Avoid for critical decisions).
*   **Explainable AI**: The system provides the "Why" behind the decision. (Required for Governance).

---

## Industry-Specific Compliance Requirements (Exam-Relevant)

Compliance is part of PMP scope management. AI decisions in regulated domains have guardrails:

| Industry | Key Regulation | AI Implication | PM Action |
|---|---|---|---|
| **Healthcare** | HIPAA | No PII/PHI in public AI tools; audit logs required | Use enterprise AI with BAA (Business Associate Agreement); sanitize all inputs |
| **Finance** | SOX, Fair Lending | Decisions must be explainable; no algorithmic bias | Use XAI; audit for bias; log all AI-influenced decisions |
| **Government** | Federal Acquisition Regulation (FAR) 52.204-21 | AI systems in contracts must meet security/transparency rules | Document AI usage in contracts; get client approval before using AI on their data |
| **Aviation/Safety** | FAA regulations | Critical safety decisions cannot be delegated to black-box AI | Use AI for analysis/prediction only; humans approve go/no-go |
| **EU/GDPR** | EU AI Act, GDPR | Users have right to explanation; data residency matters | Maintain audit trail; ensure data stays in EU if required; disclose AI usage |

---

## Practical Compliance Scenario: Healthcare Project

<strong>Scenario:</strong> Your healthcare project uses AI to optimize nursing schedules. You want to minimize burnout predictions.

<strong>Compliance Risks:</strong>
- AI might use historical bias (e.g., assigning more shifts to certain demographics)
- Patient data (PHI) touches the AI system
- Nurses have a right to understand why their shift was assigned

**PMP's Mitigation:**
1. Ensure the AI vendor has a **HIPAA Business Associate Agreement (BAA)**
2. Sanitize inputs: no patient names, medical record #s—use aggregated metrics only
3. Require **explainability**: "Nurses assigned these hours because predicted fatigue < 60% based on recent patterns"
4. Audit monthly for bias: "Are certain groups systematically assigned fewer desirable shifts?"
5. Document all AI-assisted decisions in the project log for compliance audits

---

## Bias Detection Framework

Bias doesn't just happen at deployment—it can enter at multiple stages. PMs must understand where to look.

### The Bias Lifecycle

| Stage | What Goes Wrong | PM Action |
|---|---|---|
| **Training Data** | Historical data reflects past discrimination (e.g., only promoted certain groups) | Ask vendor: "What data was the model trained on? Any known biases?" |
| **Model Design** | Algorithm optimizes for metrics that disadvantage groups (e.g., "efficiency" = overworking junior staff) | Understand success metrics; validate with diverse stakeholders |
| **Deployment Context** | Model works in lab but fails for edge cases in production | Pilot with representative sample; monitor early outcomes |
| **Feedback Loop** | Biased outputs become training data for future versions | Log corrections; don't let AI "learn" from unchallenged decisions |

### Bias Detection Checklist

- **Pre-deployment**: Review training data documentation for known gaps/biases
- **Pre-deployment**: Test model on synthetic edge cases (diverse demographics, unusual scenarios)
- **Post-deployment**: Monitor output distribution (who benefits? Who's disadvantaged?)
- **Post-deployment**: Establish feedback channel for users to report perceived unfairness
- **Ongoing**: Monthly audit of decisions for pattern analysis
- **Ongoing**: Compare AI recommendations vs. human overrides—patterns reveal bias

::: warning Exam Pattern
If a scenario asks "AI consistently recommends fewer overtime hours for one team," the correct answer involves **audit for bias, validate with diverse review, and address root cause**—never "accept because AI is objective."
:::

---

## AI Fairness Metrics (Simplified for PMs)

You don't need to calculate these yourself, but you must know when to ask for them.

| Metric | What It Measures | When to Use |
|---|---|---|
| **Disparate Impact Ratio** | `(Rate for disadvantaged group) / (Rate for advantaged group)`. If < 0.8, likely bias. | Hiring, promotions, resource allocation |
| **Equalized Odds** | Model accuracy is the same across groups (false positive/negative rates equal) | Risk scoring, loan approvals, performance predictions |
| **Demographic Parity** | Positive outcomes distributed proportionally to group representation | Selection/recommendation systems |
| **Individual Fairness** | Similar individuals receive similar outcomes | Case-by-case decisions (vendor selection, staffing) |

**Practical Example: Resource Allocation AI**
- AI recommends training budgets per team
- Team A (mostly senior engineers): $50K average
- Team B (mostly junior engineers): $15K average
- **Disparate Impact Ratio**: $15K / $50K = 0.30 (< 0.8 threshold)
- **Action**: Investigate. Is this justified (skill gap)? Or bias (investing only in already-advantaged)?

---

## Model Cards & Data Sheets (What to Ask Vendors)

When procuring AI tools, require documentation that enables governance:

### Model Card (for AI Models)
| Element | What to Ask | Why It Matters |
|---|---|---|
| **Intended Use** | "What tasks is this model designed for?" | Avoid misuse; validate fit for purpose |
| **Training Data** | "What data was used? Any known gaps?" | Understand bias sources |
| **Performance Metrics** | "How accurate is it? For which populations?" | Know where it fails |
| **Ethical Considerations** | "What risks did you identify and mitigate?" | Validate vendor responsibility |
| **Limitations** | "Where should this NOT be used?" | Avoid costly mistakes |

### Data Sheet (for Training Data)
| Element | What to Ask | Why It Matters |
|---|---|---|
| **Collection Method** | "How was data gathered? Consent obtained?" | Privacy/compliance |
| **Composition** | "What populations are represented? What's missing?" | Bias risk assessment |
| **Preprocessing** | "What cleaning/filtering was done?" | Hidden assumptions |
| **Maintenance** | "How often is data updated? Who owns it?" | Freshness and accountability |

::: tip Procurement Integration
Add Model Card requirements to your AI vendor SOW/RFP. If a vendor can't provide this documentation, that's a red flag for governance-conscious organizations.
:::

---

## The EU AI Act (2024-2026) – PM Implications

The EU AI Act is the world's first comprehensive AI law. Even non-EU projects may be affected (data from EU citizens, EU customers, global companies).

### Risk Classification System

| Risk Level | Examples | Requirements | PM Action |
|---|---|---|---|
| **Unacceptable** | Social scoring, real-time facial recognition (most cases) | **Banned** | Do not use; no exceptions |
| **High-Risk** | Hiring/HR, credit scoring, critical infrastructure, education admissions | Strict compliance: risk assessments, documentation, human oversight | Full governance framework required; document everything |
| **Limited Risk** | Chatbots, emotion recognition | Transparency obligations | Disclose AI use to users |
| **Minimal Risk** | Spam filters, game AI, recommendation engines | No specific requirements | Standard best practices |

### Key Requirements for High-Risk AI (PM Responsibilities)

1. **Risk Management System**: Document risks throughout lifecycle
2. **Data Governance**: Ensure training data is relevant, representative, error-free
3. **Technical Documentation**: Maintain records of design, development, testing
4. **Record-Keeping**: Log AI system actions for auditability
5. **Transparency**: Provide clear instructions for users
6. **Human Oversight**: Ensure humans can intervene, override, or shut down
7. **Accuracy/Robustness**: Test and validate system performance

### Timeline

| Date | Milestone |
|---|---|
| August 2024 | AI Act entered into force |
| February 2025 | Bans on unacceptable-risk AI take effect |
| August 2025 | High-risk AI obligations begin |
| August 2026 | Full enforcement for all provisions |

::: warning Exam Relevance
If a scenario mentions "AI used for hiring decisions" or "loan approval," the correct answer emphasizes **explainability, documentation, human oversight, and bias audit**—these are now legal requirements in many jurisdictions.
:::

---

## Worked Example: Bias Investigation Scenario

<strong>Scenario:</strong> Your project uses AI to rank candidates for a technical training program. After 3 months, stakeholders notice that female candidates are selected at half the rate of male candidates.

### Step 1: Contain (Don't Panic, Don't Delete)
- Pause the AI ranking system immediately
- Document current state (don't delete logs)
- Communicate to stakeholders: "We've identified a potential bias issue and are investigating"

### Step 2: Investigate Root Cause
| Hypothesis | Investigation | Finding |
|---|---|---|
| Training data bias | Review historical selection data | Historical selections were 70% male → AI learned this pattern |
| Feature bias | Check if certain features correlate with gender | "Years of experience" correlated with gender due to industry history |
| Outcome definition bias | How was "successful candidate" defined? | Defined by past manager ratings, who showed bias |

### Step 3: Quantify the Impact
- Calculate Disparate Impact Ratio: Female selection rate / Male selection rate = 0.50
- This is below the 0.8 threshold → confirms adverse impact

### Step 4: Remediate
| Action | Owner | Timeline |
|---|---|---|
| Remove "years of experience" from model inputs (or weight differently) | Data Science | 2 weeks |
| Retrain model on de-biased dataset | Data Science | 4 weeks |
| Add human review step for all selections | PM/HR | Immediate |
| Monthly bias audit going forward | PM | Ongoing |

### Step 5: Prevent Recurrence
- Update AI Working Agreement to require bias testing before deployment
- Add Disparate Impact Ratio check to Definition of Done for AI features
- Training for team on AI fairness principles

### Step 6: Communicate Resolution
- To stakeholders: "We identified bias in historical training data. We've paused the system, retrained on balanced data, added human oversight, and will audit monthly. We take fairness seriously."
- Document lessons learned for organizational knowledge base

---

## Transparency, IP, and Procurement (Exam-Relevant)
*   **Transparency**: Follow the team’s working agreement. If external stakeholders will rely on an artifact, disclose AI assistance when required and ensure a human owner signs off.
*   **IP/Copyright**: Treat prompts/outputs as potentially sensitive. Validate that your tool’s terms allow your intended commercial use and don’t accidentally leak third-party content.
*   **Procurement mindset**: If your project is buying AI capability, ensure the contract/SOW defines retention, training use, audit logs, SLAs, and responsibilities for incidents.

---

## If a Privacy/Security Incident Happens (What to Do First)
1. **Stop and contain**: halt use of the tool/workflow immediately.
2. **Notify the right parties**: IT/Security (and Legal/Compliance if required).
3. **Preserve evidence**: keep logs/screenshots per policy (don’t “cover it up”).
4. **Communicate appropriately**: follow the incident response plan for stakeholders.
5. **Prevent recurrence**: update the AI working agreement, tooling controls, and training.

<ConceptCard type="study-tip">
  <strong> Exam Insight:</strong> If a team member uses a free online AI to summarize a confidential client meeting, they have committed a <strong>Security Breach</strong>. The PM’s response is to <strong>immediately contain</strong> (notify IT/Security per policy), then educate the team and provide approved alternatives.
</ConceptCard>
